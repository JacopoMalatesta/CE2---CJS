---
title: "Report"
author: "Claudia Tang, Jacopo Malatesta"
date: "25/1/2020"
output: 
  html_document:
    toc: true
    theme: spacelab
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
```

## Loading the packages

```{r,message=FALSE}
library(RCurl); library(rvest); library(tidyverse); library(stringr);

```


## Point 1 

Inspecting the robots.txt file

```{r}
browseURL("http://beppegrillo.it/robots.txt") 
```

The first line we read says: "User-agent: *". This simply means that the following instructions apply to all robots.

The following two lines are "Disallow: /wp-admin/" and "Allow: /wp-admin/admin-ajax.php". They are telling us that the site is blocking the wp-admin folder with the exception of the admin-ajax.php file. In other words, we are not allowed to download any page from that folder with the exception of the admin-ajax.php file. 



## Point 2

Storing our URL link into an object called url 

```{r}
url <- "http://www.beppegrillo.it/un-mare-di-plastica-ci-sommergera/"

```

Downloading the page using RCcurl::getURL() while informing the webmaster about my browser details and my e-mail address.

```{r}
page <- getURL(url, 
               useragent = str_c(R.version$platform,
                                 R.version$version.string,
                                 sep = ", "),
               httpheader = c(From = "jacopomalatesta95@gmail.com")) 

```


Saving the page in my directory

```{r}
writeLines(page, 
           con = here::here("downloaded_pages", "beppe_grillo_plastica.html")) 

```


## Point 3

Downloading all the links in the page 

```{r}
links <- XML::getHTMLLinks( here::here("downloaded_pages","beppe_grillo_plastica.html"))

```

Creating a tibble to store all the links

```{r}
dat <- tibble(
  links = links
)

head(dat)

```

Selecting all the links re-directing to other posts from the beppegrillo.it blog

```{r}
internal_links <- str_subset(links, "^http://www\\.beppegrillo\\.it")

```

Creating a tibble with all the links re-directing to other posts from the beppegrillo.it blog

```{r}
dat2 <- tibble(
  links = internal_links
)

head(dat2)

```


Same thing, this time with rvest

```{r}
links_rvest <- read_html(here::here("downloaded_pages", "beppe_grillo_plastica.html")) %>% 
  html_nodes(css = "a") %>% 
  html_attr("href")

```


Creating a dataframe containing the links

```{r}
dat3 <- tibble(
  links = links_rvest
)

head(dat3)

```


Selecting all the links re-directing to other posts from the beppegrillo.it blog

```{r}
internal_links_rvest <- str_subset(links, "^http://www\\.beppegrillo\\.it")

```

Creating a dataframe containing the links re-directing to other posts from the beppegrillo.it blog

```{r}
dat4 <- tibble(
  links = internal_links_rvest
)

head(dat4)
```


## Point 6

Crawling means looking at all the content and codes on a page, analyzing and downloading them. This can be done for 
different purposes, the main one being web indexing. A web spider is a program that automatically browses and downloads Web pages by following hyperlinks in a methodical and automated manner. They are usually used for web indexing, but 
can also be used for web scraping. 


---
title: "CE2 - sdb trial"
author: "simone de battisti"
output:
  html_document:
    df_print: paged
---

# 1 wwww.beppegrillo.it Inspect the robot.txt and describe what you can and what you should not do. Pay attention to the allow / disallow statements and the definition of user-agent. What do these lines mean?

```r

url_BG <- "wwww.beppegrillo.it"


library(tidyverse)
library(rvest)
library(here)
#library(robotstxt)

robots_BG <- "www.beppegrillo.it/robots.txt"
browseURL(robots_BG)



#rt_get_useragent(robots_BG)
#i wanted to dowload the robot.txt to print it directly but i didnt managed
```

The file leaves pretty open access for downloading and scraping. not particular rules of politeness, maybe they would like to be used (more use more ads?).   we will be polite anycase. they also indicate the sitemap structure location.


#2 Check out the following link: http://www.beppegrillo.it/un-mare-di-plastica-ci-sommergera/. Download it using RCcurl::getURL() to download the page while informing the webmaster about your browser details and providing your email.

Use URLencode() to avoid potential problems with formatting the URL.

```{r}
library(RCurl);


url_plastica <- URLencode("http://www.beppegrillo.it/un-mare-di-plastica-ci-sommergera")


# Downloading the file:
download.file(url = url_plastica,                       
               destfile = here::here("/download/plastica.html"))

plastica <- getURL(url_plastica, 
       useragent = str_c(R.version$platform,
                         R.version$version.string,
                         sep = ", "),
       httpheader = c(From = "simone@ospo.it"))

#dimensione diversa da download.file
writeLines(url_plastica, 
           con = here::here("/download/plastica_get.html"))


browseURL(url_plastica)
```

after preparation we go for the scraping. selectog gadged show "span a" as css selector.

```r

plastica_cont <- read_html(here::here("/download/plastica_get.html")) %>% 
     html_nodes(css = "span a") %>% 
     html_text()
     
plastica_cont

#but its empty

```


#Create a data frame with all the HTML links in the page using the XML::getHTMLLinks(). Then, use a regex to keep only those links that re-direct to other posts of the beppegrillo.it blog (so remove all other links). Finally, achieve the same result using rvest:: instead of XML.


I searched in the helper as i could not find this funtion used in class. then i tried the class version but uinsucessfully (cant really find the attr)

so hard to scrape using a lopp even if the master doesnt work.


```{r}


getHTMLLinks(here::here (/download/plastica.html), externalOnly = TRUE, xpQuery = "//a/@href",
               baseURL = plastica.html, relative = FALSE)

# Tried this also unsucessfully
# links <- read_html(here::here("/download/plastica_get.html")) %>%
# html_nodes(css = "span a") %>% 
# html_attr('td-outer-container')    #provato anche con "a" "href"
# links


#maybe depends on the html file saved?

links <- read_html(here::here("/download/plastica_get.html")) %>%
html_nodes(css = "span a") %>% 
html_attr('href')    
links

```


#4 Go back to the initial link and focus on the bottom of the page: "Prossimo articolo" it means following article. Scrape this link and then use it to scrape the article "In Svizzera il tragitto casa-ufficio è orario di lavoro” (i.e. the following page). How could you use these previous and following links to scrape many more blog posts? [don’t do it, just sketch the ideas and the R functions you should use]


links_prossimoarticolo <- read_html(here::here("/download/plastica_get.html")) %>%
html_nodes(css = ".td-post-next-post a") %>% 
html_attr('href') 
links



#5 Check out the following link: http://www.beppegrillo.it/category/archivio/2016/. It contains the entire blog for 2016. There are 47 pages of entries. Scrape all the posts for 2016 following this strategy:
For each of the 47 pages, get all the links and place them into a list (or character vector)
b. For each link, download the files and sys.sleep() for few seconds
c. For each downloaded page, scrape the main text. Ask yourself what happens if a page contains no text.


per non disturbare troppo prima scarichiamo la pagina

```r
url_5 <- "http://www.beppegrillo.it/category/archivio/2016"

browseURL(url_5)


# Let's check quickly:
texturl5 <- read_html(url_5) %>% 
     html_nodes(css = ".td-outer-container , .td-module-title a") %>% 
     html_text()


texturl5






